{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f77e9ccb",
   "metadata": {},
   "source": [
    "# Deploying Quantization Aware Trained models in INT8 using Torch-TensorRT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21645638",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Quantization Aware training (QAT) simulates quantization during training by quantizing weights and activation layers. This will help to reduce the loss in accuracy when we convert the network trained in FP32 to INT8 for faster inference. QAT introduces additional nodes in the graph which will be used to learn the dynamic ranges of weights and activation layers. In this notebook, we illustrate the following steps from training to inference of a QAT model in Torch-TensorRT.\n",
    "\n",
    "1. [Requirements](#1)\n",
    "2. [VGG16 Overview](#2)\n",
    "3. [Training a baseline VGG16 model](#3)\n",
    "4. [Apply Quantization](#4)\n",
    "5. [Model calibration](#5)\n",
    "6. [Quantization Aware training](#6)\n",
    "7. [Export to Torchscript](#7)\n",
    "8. [Inference using Torch-TensorRT](#8)\n",
    "8. [References](#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a88c4c",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "##  1. Requirements\n",
    "Please install the <a href=\"https://github.com/NVIDIA/Torch-TensorRT/tree/master/examples/int8/training/vgg16#prequisites\">required dependencies</a> and import these libraries accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145aed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch_tensorrt\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import quant_modules\n",
    "from pytorch_quantization.tensor_quant import QuantDescriptor\n",
    "from pytorch_quantization import calib\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, \"../examples/int8/training/vgg16\")\n",
    "from vgg16 import vgg16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2253b9",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "##  2. VGG16 Overview\n",
    "### Very Deep Convolutional Networks for Large-Scale Image Recognition\n",
    "VGG is one of the earliest family of image classification networks that first used small (3x3) convolution filters and achieved significant improvements on ImageNet recognition challenge. The network architecture looks as follows\n",
    "<img src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png\">\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d92fba",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "##  3. Training a baseline VGG16 model\n",
    "We train VGG16 on CIFAR10 dataset. Define training and testing datasets and dataloaders. This will download the CIFAR 10 data in your `data` directory. Data preprocessing is performed using `torchvision` transforms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00865744",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# ========== Define Training dataset and dataloaders =============#\n",
    "training_dataset = datasets.CIFAR10(root='./data',\n",
    "                                        train=True,\n",
    "                                        download=True,\n",
    "                                        transform=transforms.Compose([\n",
    "                                            transforms.RandomCrop(32, padding=4),\n",
    "                                            transforms.RandomHorizontalFlip(),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "                                        ]))\n",
    "\n",
    "training_dataloader = torch.utils.data.DataLoader(training_dataset,\n",
    "                                                      batch_size=32,\n",
    "                                                      shuffle=True,\n",
    "                                                      num_workers=2)\n",
    "\n",
    "# ========== Define Testing dataset and dataloaders =============#\n",
    "testing_dataset = datasets.CIFAR10(root='./data',\n",
    "                                   train=False,\n",
    "                                   download=True,\n",
    "                                   transform=transforms.Compose([\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "                                   ]))\n",
    "\n",
    "testing_dataloader = torch.utils.data.DataLoader(testing_dataset,\n",
    "                                                 batch_size=16,\n",
    "                                                 shuffle=False,\n",
    "                                                 num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc575d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, crit, opt, epoch):\n",
    "#     global writer\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch, (data, labels) in enumerate(dataloader):\n",
    "        data, labels = data.cuda(), labels.cuda(non_blocking=True)\n",
    "        opt.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = crit(out, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if batch % 500 == 499:\n",
    "            print(\"Batch: [%5d | %5d] loss: %.3f\" % (batch + 1, len(dataloader), running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "        \n",
    "def test(model, dataloader, crit, epoch):\n",
    "    global writer\n",
    "    global classes\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss = 0.0\n",
    "    class_probs = []\n",
    "    class_preds = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, labels in dataloader:\n",
    "            data, labels = data.cuda(), labels.cuda(non_blocking=True)\n",
    "            out = model(data)\n",
    "            loss += crit(out, labels)\n",
    "            preds = torch.max(out, 1)[1]\n",
    "            class_probs.append([F.softmax(i, dim=0) for i in out])\n",
    "            class_preds.append(preds)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "    test_probs = torch.cat([torch.stack(batch) for batch in class_probs])\n",
    "    test_preds = torch.cat(class_preds)\n",
    "\n",
    "    return loss / total, correct / total\n",
    "\n",
    "def save_checkpoint(state, ckpt_path=\"checkpoint.pth\"):\n",
    "    torch.save(state, ckpt_path)\n",
    "    print(\"Checkpoint saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33343eaa",
   "metadata": {},
   "source": [
    "*Define the VGG model that we are going to perfom QAT on.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c734cd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR 10 has 10 classes\n",
    "model = vgg16(num_classes=len(classes), init_weights=False)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc77ca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare Learning rate\n",
    "lr = 0.1\n",
    "state = {}\n",
    "state[\"lr\"] = lr\n",
    "\n",
    "# Use cross entropy loss for classification and SGD optimizer\n",
    "crit = nn.CrossEntropyLoss()\n",
    "opt = optim.SGD(model.parameters(), lr=state[\"lr\"], momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "# Adjust learning rate based on epoch number\n",
    "def adjust_lr(optimizer, epoch):\n",
    "    global state\n",
    "    new_lr = lr * (0.5**(epoch // 12)) if state[\"lr\"] > 1e-7 else state[\"lr\"]\n",
    "    if new_lr != state[\"lr\"]:\n",
    "        state[\"lr\"] = new_lr\n",
    "        print(\"Updating learning rate: {}\".format(state[\"lr\"]))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = state[\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb40c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for 25 epochs to get ~80% accuracy.\n",
    "num_epochs=25\n",
    "for epoch in range(num_epochs):\n",
    "    adjust_lr(opt, epoch)\n",
    "    print('Epoch: [%5d / %5d] LR: %f' % (epoch + 1, num_epochs, state[\"lr\"]))\n",
    "\n",
    "    train(model, training_dataloader, crit, opt, epoch)\n",
    "    test_loss, test_acc = test(model, testing_dataloader, crit, epoch)\n",
    "\n",
    "    print(\"Test Loss: {:.5f} Test Acc: {:.2f}%\".format(test_loss, 100 * test_acc))\n",
    "    \n",
    "save_checkpoint({'epoch': epoch + 1,\n",
    "                 'model_state_dict': model.state_dict(),\n",
    "                 'acc': test_acc,\n",
    "                 'opt_state_dict': opt.state_dict(),\n",
    "                 'state': state},\n",
    "                ckpt_path=\"vgg16_base_ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dea8c09",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "##  4. Apply Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250766a7",
   "metadata": {},
   "source": [
    "`quant_modules.initialize()` will ensure quantized version of modules will be called instead of original modules. For example, when you define a model with convolution, linear, pooling layers, `QuantConv2d`, `QuantLinear` and `QuantPooling` will be called. `QuantConv2d` basically wraps quantizer nodes around inputs and weights of regular `Conv2d`. Please refer to all the <a href=\"https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization/pytorch_quantization/nn/modules\">quantized modules</a> in pytorch-quantization toolkit for more information. A `QuantConv2d` is represented in `pytorch-quantization` toolkit as follows.\n",
    "\n",
    "```\n",
    "def forward(self, input):\n",
    "        # the actual quantization happens in the next level of the class hierarchy\n",
    "        quant_input, quant_weight = self._quant(input)\n",
    "\n",
    "        if self.padding_mode == 'circular':\n",
    "            expanded_padding = ((self.padding[1] + 1) // 2, self.padding[1] // 2,\n",
    "                                (self.padding[0] + 1) // 2, self.padding[0] // 2)\n",
    "            output = F.conv2d(F.pad(quant_input, expanded_padding, mode='circular'),\n",
    "                              quant_weight, self.bias, self.stride,\n",
    "                              _pair(0), self.dilation, self.groups)\n",
    "        else:\n",
    "            output = F.conv2d(quant_input, quant_weight, self.bias, self.stride, self.padding, self.dilation,\n",
    "                              self.groups)\n",
    "\n",
    "        return output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f1be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_modules.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeeb74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the regular conv, FC layers will be converted to their quantozed counterparts due to quant_modules.initialize()\n",
    "qat_model = vgg16(num_classes=len(classes), init_weights=False)\n",
    "qat_model = qat_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3245eba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg16_base_ckpt is the checkpoint generated from Step 3 : Training a baseline VGG16 model.\n",
    "ckpt = torch.load(\"./vgg16_base_ckpt\")\n",
    "modified_state_dict={}\n",
    "for key, val in ckpt[\"model_state_dict\"].items():\n",
    "    # Remove 'module.' from the key names\n",
    "    if key.startswith('module'):\n",
    "        modified_state_dict[key[7:]] = val\n",
    "    else:\n",
    "        modified_state_dict[key] = val\n",
    "\n",
    "# Load the pre-trained checkpoint\n",
    "qat_model.load_state_dict(modified_state_dict)\n",
    "opt.load_state_dict(ckpt[\"opt_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4162f44f",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "##  5. Model Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c0c5fc",
   "metadata": {},
   "source": [
    "The quantizer nodes introduced in the model around desired layers capture the dynamic range (min_value, max_value) that is observed by the layer. Calibration is the process of computing the dynamic range of these layers by passing calibration data, which is usually a subset of training or validation data. There are different ways of calibration: `max`, `histogram` and `entropy`. We use `max` calibration technique as it is simple and effective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d700cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_amax(model, **kwargs):\n",
    "    # Load calib result\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                if isinstance(module._calibrator, calib.MaxCalibrator):\n",
    "                    module.load_calib_amax()\n",
    "                else:\n",
    "                    module.load_calib_amax(**kwargs)\n",
    "            print(F\"{name:40}: {module}\")\n",
    "    model.cuda()\n",
    "\n",
    "def collect_stats(model, data_loader, num_batches):\n",
    "    \"\"\"Feed data to the network and collect statistics\"\"\"\n",
    "    # Enable calibrators\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                module.disable_quant()\n",
    "                module.enable_calib()\n",
    "            else:\n",
    "                module.disable()\n",
    "\n",
    "    # Feed data to the network for collecting stats\n",
    "    for i, (image, _) in tqdm(enumerate(data_loader), total=num_batches):\n",
    "        model(image.cuda())\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "\n",
    "    # Disable calibrators\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                module.enable_quant()\n",
    "                module.disable_calib()\n",
    "            else:\n",
    "                module.enable()\n",
    "\n",
    "def calibrate_model(model, model_name, data_loader, num_calib_batch, calibrator, hist_percentile, out_dir):\n",
    "    \"\"\"\n",
    "        Feed data to the network and calibrate.\n",
    "        Arguments:\n",
    "            model: classification model\n",
    "            model_name: name to use when creating state files\n",
    "            data_loader: calibration data set\n",
    "            num_calib_batch: amount of calibration passes to perform\n",
    "            calibrator: type of calibration to use (max/histogram)\n",
    "            hist_percentile: percentiles to be used for historgram calibration\n",
    "            out_dir: dir to save state files in\n",
    "    \"\"\"\n",
    "\n",
    "    if num_calib_batch > 0:\n",
    "        print(\"Calibrating model\")\n",
    "        with torch.no_grad():\n",
    "            collect_stats(model, data_loader, num_calib_batch)\n",
    "\n",
    "        if not calibrator == \"histogram\":\n",
    "            compute_amax(model, method=\"max\")\n",
    "            calib_output = os.path.join(\n",
    "                out_dir,\n",
    "                F\"{model_name}-max-{num_calib_batch*data_loader.batch_size}.pth\")\n",
    "            torch.save(model.state_dict(), calib_output)\n",
    "        else:\n",
    "            for percentile in hist_percentile:\n",
    "                print(F\"{percentile} percentile calibration\")\n",
    "                compute_amax(model, method=\"percentile\")\n",
    "                calib_output = os.path.join(\n",
    "                    out_dir,\n",
    "                    F\"{model_name}-percentile-{percentile}-{num_calib_batch*data_loader.batch_size}.pth\")\n",
    "                torch.save(model.state_dict(), calib_output)\n",
    "\n",
    "            for method in [\"mse\", \"entropy\"]:\n",
    "                print(F\"{method} calibration\")\n",
    "                compute_amax(model, method=method)\n",
    "                calib_output = os.path.join(\n",
    "                    out_dir,\n",
    "                    F\"{model_name}-{method}-{num_calib_batch*data_loader.batch_size}.pth\")\n",
    "                torch.save(model.state_dict(), calib_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201e8b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calibrate the model using max calibration technique.\n",
    "with torch.no_grad():\n",
    "    calibrate_model(\n",
    "        model=qat_model,\n",
    "        model_name=\"vgg16\",\n",
    "        data_loader=training_dataloader,\n",
    "        num_calib_batch=32,\n",
    "        calibrator=\"max\",\n",
    "        hist_percentile=[99.9, 99.99, 99.999, 99.9999],\n",
    "        out_dir=\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806f343d",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "##  6. Quantization Aware Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60601993",
   "metadata": {},
   "source": [
    "In this phase, we finetune the model weights and leave the quantizer node values frozen. The dynamic ranges for each layer obtained from the calibration are kept constant while the weights of the model are finetuned to be close to the accuracy of original FP32 model (model without quantizer nodes) is preserved. Usually the finetuning of QAT model should be quick compared to the full training of the original model. Use QAT to fine-tune for around 10% of the original training schedule with an annealing learning-rate. Please refer to <a href=\"https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt/\">Achieving FP32 Accuracy for INT8 Inference Using Quantization Aware Training with NVIDIA TensorRT</a> for detailed recommendations. For this VGG model, it is enough to finetune for 1 epoch to get acceptable accuracy. \n",
    "During finetuning with QAT, the quantization is applied as a composition of `max`, `clamp`, `round` and `mul` ops. \n",
    "```\n",
    "# amax is absolute maximum value for an input\n",
    "# The upper bound for integer quantization (127 for int8)\n",
    "max_bound = torch.tensor((2.0**(num_bits - 1 + int(unsigned))) - 1.0, device=amax.device)\n",
    "scale = max_bound / amax\n",
    "outputs = torch.clamp((inputs * scale).round_(), min_bound, max_bound)\n",
    "```\n",
    "<a href=\"https://github.com/NVIDIA/TensorRT/blob/8.0.1/tools/pytorch-quantization/pytorch_quantization/tensor_quant.py\">tensor_quant function</a> in `pytorch_quantization` toolkit is responsible for the above tensor quantization. Usually, per channel quantization is recommended for weights, while per tensor quantization is recommended for activations in a network.\n",
    "During inference, we use `torch.fake_quantize_per_tensor_affine` and `torch.fake_quantize_per_channel_affine` to perform quantization as this is easier to convert into corresponding TensorRT operators. Please refer to next sections for more details on how these operators are exported in torchscript and converted in Torch-TensorRT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d266225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune the QAT model for 1 epoch\n",
    "num_epochs=1\n",
    "for epoch in range(num_epochs):\n",
    "    adjust_lr(opt, epoch)\n",
    "    print('Epoch: [%5d / %5d] LR: %f' % (epoch + 1, num_epochs, state[\"lr\"]))\n",
    "\n",
    "    train(qat_model, training_dataloader, crit, opt, epoch)\n",
    "    test_loss, test_acc = test(qat_model, testing_dataloader, crit, epoch)\n",
    "\n",
    "    print(\"Test Loss: {:.5f} Test Acc: {:.2f}%\".format(test_loss, 100 * test_acc))\n",
    "    \n",
    "save_checkpoint({'epoch': epoch + 1,\n",
    "                 'model_state_dict': qat_model.state_dict(),\n",
    "                 'acc': test_acc,\n",
    "                 'opt_state_dict': opt.state_dict(),\n",
    "                 'state': state},\n",
    "                ckpt_path=\"vgg16_qat_ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b012fd",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "##  7. Export to Torchscript\n",
    "Export the model to Torch script. Trace the model and convert it into torchscript for deployment. To learn more about Torchscript, please refer to https://pytorch.org/docs/stable/jit.html. Setting `quant_nn.TensorQuantizer.use_fb_fake_quant = True` enables the QAT model to use `torch.fake_quantize_per_tensor_affine` and `torch.fake_quantize_per_channel_affine` operators instead of `tensor_quant` function to export quantization operators. In torchscript, they are represented as `aten::fake_quantize_per_tensor_affine` and `aten::fake_quantize_per_channel_affine`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f104098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_nn.TensorQuantizer.use_fb_fake_quant = True\n",
    "with torch.no_grad():\n",
    "    data = iter(testing_dataloader)\n",
    "    images, _ = data.next()\n",
    "    jit_model = torch.jit.trace(qat_model, images.to(\"cuda\"))\n",
    "    torch.jit.save(jit_model, \"trained_vgg16_qat.jit.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8190d3",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>\n",
    "##  8. Inference using Torch-TensorRT\n",
    "In this phase, we run the exported torchscript graph of VGG QAT using Torch-TensorRT. Torch-TensorRT is a Pytorch-TensorRT compiler which converts Torchscript graphs into TensorRT. TensorRT 8.0 supports inference of quantization aware trained models and introduces new APIs; `QuantizeLayer` and `DequantizeLayer`. We can observe the entire VGG QAT graph quantization nodes from the debug log of Torch-TensorRT. To enable debug logging, you can set `torch_tensorrt.logging.set_reportable_log_level(torch_tensorrt.logging.Level.Debug)`. For example, `QuantConv2d` layer from `pytorch_quantization` toolkit is represented as follows in Torchscript\n",
    "```\n",
    "%quant_input : Tensor = aten::fake_quantize_per_tensor_affine(%x, %636, %637, %638, %639)\n",
    "%quant_weight : Tensor = aten::fake_quantize_per_channel_affine(%394, %640, %641, %637, %638, %639)\n",
    "%input.2 : Tensor = aten::_convolution(%quant_input, %quant_weight, %395, %687, %688, %689, %643, %690, %642, %643, %643, %644, %644)\n",
    "```\n",
    "`aten::fake_quantize_per_*_affine` is converted into `QuantizeLayer` + `DequantizeLayer` in Torch-TensorRT internally. Please refer to <a href=\"https://github.com/NVIDIA/Torch-TensorRT/blob/master/core/conversion/converters/impl/quantization.cpp\">quantization op converters</a> in Torch-TensorRT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f3ac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qat_model = torch.jit.load(\"trained_vgg16_qat.jit.pt\").eval()\n",
    "\n",
    "compile_spec = {\"inputs\": [torch_tensorrt.Input([16, 3, 32, 32])],\n",
    "                \"enabled_precisions\": torch.int8,\n",
    "                }\n",
    "trt_mod = torch_tensorrt.compile(qat_model, **compile_spec)\n",
    "\n",
    "test_loss, test_acc = test(trt_mod, testing_dataloader, crit, 0)\n",
    "print(\"VGG QAT accuracy using TensorRT: {:.2f}%\".format(100 * test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6164fcde",
   "metadata": {},
   "source": [
    "### Performance benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b1aac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Helper function to benchmark the model\n",
    "def benchmark(model, input_shape=(1024, 1, 32, 32), dtype='fp32', nwarmup=50, nruns=1000):\n",
    "    input_data = torch.randn(input_shape)\n",
    "    input_data = input_data.to(\"cuda\")\n",
    "    if dtype=='fp16':\n",
    "        input_data = input_data.half()\n",
    "        \n",
    "    print(\"Warm up ...\")\n",
    "    with torch.no_grad():\n",
    "        for _ in range(nwarmup):\n",
    "            features = model(input_data)\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"Start timing ...\")\n",
    "    timings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(1, nruns+1):\n",
    "            start_time = time.time()\n",
    "            output = model(input_data)\n",
    "            torch.cuda.synchronize()\n",
    "            end_time = time.time()\n",
    "            timings.append(end_time - start_time)\n",
    "            if i%100==0:\n",
    "                print('Iteration %d/%d, avg batch time %.2f ms'%(i, nruns, np.mean(timings)*1000))\n",
    "\n",
    "    print(\"Input shape:\", input_data.size())\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print('Average batch time: %.2f ms'%(np.mean(timings)*1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f45a0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(jit_model, input_shape=(16, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e383d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(trt_mod, input_shape=(16, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4747f767",
   "metadata": {},
   "source": [
    "<a id=\"9\"></a>\n",
    "##  9. References\n",
    "* <a href=\"https://arxiv.org/pdf/1409.1556.pdf\">Very Deep Convolution Networks for large scale Image Recognition</a>\n",
    "* <a href=\"https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt/\">Achieving FP32 Accuracy for INT8 Inference Using Quantization Aware Training with NVIDIA TensorRT</a>\n",
    "* <a href=\"https://github.com/NVIDIA/Torch-TensorRT/tree/master/examples/int8/training/vgg16#quantization-aware-fine-tuning-for-trying-out-qat-workflows\">QAT workflow for VGG16</a>\n",
    "* <a href=\"https://github.com/NVIDIA/Torch-TensorRT/tree/master/examples/int8/qat\">Deploying VGG QAT model in C++ using Torch-TensorRT</a>\n",
    "* <a href=\"https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization\">Pytorch-quantization toolkit from NVIDIA</a>\n",
    "* <a href=\"https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/userguide.html\">Pytorch quantization toolkit userguide</a>\n",
    "* <a href=\"https://arxiv.org/pdf/2004.09602.pdf\">Quantization basics</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
