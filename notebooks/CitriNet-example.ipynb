{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Torch-TensorRT Getting Started - CitriNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "[Citrinet](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/models.html#citrinet) is an acoustic model used for the speech to text recognition task. It is a version of [QuartzNet](https://arxiv.org/pdf/1910.10261.pdf) that extends [ContextNet](https://arxiv.org/pdf/2005.03191.pdf), utilizing subword encoding (via Word Piece tokenization) and Squeeze-and-Excitation(SE) mechanism and are therefore smaller than QuartzNet models.\n",
    "\n",
    "CitriNet models take in audio segments and transcribe them to letter, byte pair, or word piece sequences. \n",
    "\n",
    "<img src=\"https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/_images/jasper_vertical.png\" alt=\"alt\" width=\"50%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning objectives\n",
    "\n",
    "This notebook demonstrates the steps for optimizing a pretrained CitriNet model with Torch-TensorRT, and running it to test the speedup obtained.\n",
    "\n",
    "## Content\n",
    "1. [Requirements](#1)\n",
    "1. [Download Citrinet model](#2)\n",
    "1. [Create Torch-TensorRT modules](#3)\n",
    "1. [Benchmark Torch-TensorRT models](#4)\n",
    "1. [Conclusion](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. Requirements\n",
    "\n",
    "Follow the steps in [README](README.md) to prepare a Docker container, within which you can run this notebook. \n",
    "This notebook assumes that you are within a Jupyter environment in a docker container with Torch-TensorRT installed, such as an NGC monthly release of `nvcr.io/nvidia/pytorch:<yy.mm>-py3` (where `yy` indicates the last two numbers of a calendar year, and `mm` indicates the month in two-digit numerical form)\n",
    "\n",
    "Now that you are in the docker, the next step is to install the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install wget\n",
    "!apt-get update && DEBIAN_FRONTEND=noninteractive  apt-get install -y libsndfile1 ffmpeg\n",
    "!pip install Cython\n",
    "\n",
    "## Install NeMo\n",
    "!pip install nemo_toolkit[all]==1.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. Download Citrinet model\n",
    "\n",
    "Next, we download a pretrained Nemo Citrinet model and convert it to a Torchscript module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo\n",
    "import torch\n",
    "\n",
    "import nemo.collections.asr as nemo_asr\n",
    "from nemo.core import typecheck\n",
    "typecheck.set_typecheck_enabled(False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant = 'stt_en_citrinet_256'\n",
    "\n",
    "print(f\"Downloading and saving {variant}...\")\n",
    "asr_model = nemo_asr.models.EncDecCTCModelBPE.from_pretrained(model_name=variant)\n",
    "asr_model.export(f\"{variant}.ts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark utility\n",
    "\n",
    "Let us define a helper benchmarking function, then benchmark the original Pytorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import argparse\n",
    "import timeit\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch_tensorrt as trtorch\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "def benchmark(model, input_tensor, num_loops, model_name, batch_size):\n",
    "    def timeGraph(model, input_tensor, num_loops):\n",
    "        print(\"Warm up ...\")\n",
    "        with torch.no_grad():\n",
    "            for _ in range(20):\n",
    "                features = model(input_tensor)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        print(\"Start timing ...\")\n",
    "        timings = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(num_loops):\n",
    "                start_time = timeit.default_timer()\n",
    "                features = model(input_tensor)\n",
    "                torch.cuda.synchronize()\n",
    "                end_time = timeit.default_timer()\n",
    "                timings.append(end_time - start_time)\n",
    "                # print(\"Iteration {}: {:.6f} s\".format(i, end_time - start_time))\n",
    "        return timings\n",
    "    def printStats(graphName, timings, batch_size):\n",
    "        times = np.array(timings)\n",
    "        steps = len(times)\n",
    "        speeds = batch_size / times\n",
    "        time_mean = np.mean(times)\n",
    "        time_med = np.median(times)\n",
    "        time_99th = np.percentile(times, 99)\n",
    "        time_std = np.std(times, ddof=0)\n",
    "        speed_mean = np.mean(speeds)\n",
    "        speed_med = np.median(speeds)\n",
    "        msg = (\"\\n%s =================================\\n\"\n",
    "                \"batch size=%d, num iterations=%d\\n\"\n",
    "                \"  Median samples/s: %.1f, mean: %.1f\\n\"\n",
    "                \"  Median latency (s): %.6f, mean: %.6f, 99th_p: %.6f, std_dev: %.6f\\n\"\n",
    "                ) % (graphName,\n",
    "                    batch_size, steps,\n",
    "                    speed_med, speed_mean,\n",
    "                    time_med, time_mean, time_99th, time_std)\n",
    "        print(msg)\n",
    "    timings = timeGraph(model, input_tensor, num_loops)\n",
    "    printStats(model_name, timings, batch_size)\n",
    "\n",
    "precisions_str = 'fp32' # Precision (default=fp32, fp16)\n",
    "variant = 'stt_en_citrinet_256' # Nemo Citrinet variant\n",
    "batch_sizes = [1, 8, 32, 128] # Batch sizes (default=1,8,32,128)\n",
    "trt = False # If True, infer with Torch-TensorRT engine. Else, infer with Pytorch model.\n",
    "precision = torch.float32 if precisions_str =='fp32' else torch.float16\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    if trt:\n",
    "        model_name = f\"{variant}_bs{batch_size}_{precision}.torch-tensorrt\"\n",
    "    else:\n",
    "        model_name = f\"{variant}.ts\"\n",
    "\n",
    "    print(f\"Loading model: {model_name}\") \n",
    "    # Load traced model to CPU first\n",
    "    model = torch.jit.load(model_name).cuda()\n",
    "    cudnn.benchmark = True\n",
    "    # Create random input tensor of certain size\n",
    "    torch.manual_seed(12345)\n",
    "    input_shape=(batch_size, 80, 1488)\n",
    "    input_tensor = torch.randn(input_shape).cuda()\n",
    "\n",
    "    # Timing graph inference\n",
    "    benchmark(model, input_tensor, 50, model_name, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirming the GPU we are using here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. Create Torch-TensorRT modules\n",
    "\n",
    "In this step, we optimize the Citrinet Torchscript module with Torch-TensorRT with various precisions and batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_tensorrt as trtorch\n",
    "import argparse\n",
    "\n",
    "# trtorch.logging.set_reportable_log_level(trtorch.logging.Level.Info)\n",
    "\n",
    "arg_precisions = \"fp32,fp16\"\n",
    "arg_batch_sizes = \"1,8,32,128\"\n",
    "arg_variant = \"stt_en_citrinet_256\"\n",
    "\n",
    "precisions_str = arg_precisions.split(',')\n",
    "precisions = []\n",
    "if 'fp32' in precisions_str:\n",
    "    precisions.append(torch.float32)\n",
    "if 'fp16' in precisions_str:\n",
    "    precisions.append(torch.half)\n",
    "\n",
    "batch_sizes = [int(x) for x in arg_batch_sizes.split(',')]\n",
    "\n",
    "model = torch.jit.load(f\"{arg_variant}.ts\")\n",
    "\n",
    "for precision in precisions:\n",
    "    for batch_size in batch_sizes:\n",
    "        compile_settings = {\n",
    "            \"inputs\": [trtorch.Input(shape=[batch_size, 80, 1488])],\n",
    "            \"enabled_precisions\": {precision},\n",
    "            \"workspace_size\": 2000000000,\n",
    "            \"truncate_long_and_double\": True,\n",
    "        }\n",
    "        print(f\"Generating Torchscript-TensorRT module for batchsize {batch_size} precision {precision}\")\n",
    "        trt_ts_module = trtorch.compile(model, **compile_settings)\n",
    "        torch.jit.save(trt_ts_module, f\"{arg_variant}_bs{batch_size}_{precision}.torch-tensorrt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4. Benchmark Torch-TensorRT models\n",
    "\n",
    "Finally, we are ready to benchmark the Torch-TensorRT optimized Citrinet models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP32 (single precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions_str = 'fp32' # Precision (default=fp32, fp16)\n",
    "batch_sizes = [1, 8, 32, 128] # Batch sizes (default=1,8,32,128)\n",
    "precision = torch.float32 if precisions_str =='fp32' else torch.float16\n",
    "trt = True\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    if trt:\n",
    "        model_name = f\"{variant}_bs{batch_size}_{precision}.torch-tensorrt\"\n",
    "    else:\n",
    "        model_name = f\"{variant}.ts\"\n",
    "\n",
    "    print(f\"Loading model: {model_name}\") \n",
    "    # Load traced model to CPU first\n",
    "    model = torch.jit.load(model_name).cuda()\n",
    "    cudnn.benchmark = True\n",
    "    # Create random input tensor of certain size\n",
    "    torch.manual_seed(12345)\n",
    "    input_shape=(batch_size, 80, 1488)\n",
    "    input_tensor = torch.randn(input_shape).cuda()\n",
    "\n",
    "    # Timing graph inference\n",
    "    benchmark(model, input_tensor, 50, model_name, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP16 (half precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions_str = 'fp16' # Precision (default=fp32, fp16)\n",
    "batch_sizes = [1, 8, 32, 128] # Batch sizes (default=1,8,32,128)\n",
    "precision = torch.float32 if precisions_str =='fp32' else torch.float16\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    if trt:\n",
    "        model_name = f\"{variant}_bs{batch_size}_{precision}.torch-tensorrt\"\n",
    "    else:\n",
    "        model_name = f\"{variant}.ts\"\n",
    "\n",
    "    print(f\"Loading model: {model_name}\") \n",
    "    # Load traced model to CPU first\n",
    "    model = torch.jit.load(model_name).cuda()\n",
    "    cudnn.benchmark = True\n",
    "    # Create random input tensor of certain size\n",
    "    torch.manual_seed(12345)\n",
    "    input_shape=(batch_size, 80, 1488)\n",
    "    input_tensor = torch.randn(input_shape).cuda()\n",
    "\n",
    "    # Timing graph inference\n",
    "    benchmark(model, input_tensor, 50, model_name, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## 5. Conclusion\n",
    "\n",
    "In this notebook, we have walked through the complete process of optimizing the Citrinet model with Torch-TensorRT. On an A100 GPU, with Torch-TensorRT, we observe a speedup of ~**2.4X** with FP32, and ~**2.9X** with FP16 at batchsize of 128.\n",
    "\n",
    "### What's next\n",
    "Now it's time to try Torch-TensorRT on your own model. Fill out issues at https://github.com/NVIDIA/Torch-TensorRT. Your involvement will help future development of Torch-TensorRT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
